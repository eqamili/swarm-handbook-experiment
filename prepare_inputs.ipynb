{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77d9b8a-d967-4091-8662-bf95842d69c8",
   "metadata": {},
   "source": [
    "# Prepare Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d79b98-deeb-4b81-921c-83775ed3e46f",
   "metadata": {},
   "source": [
    "This notebook fetches the external data and updates the copies stored in this repository.\n",
    "\n",
    "1. Fetch the Google Sheet containing the list of products and descriptions\n",
    "   - Store the data in `input/overview.csv`\n",
    "2. Fetch example product files for each product and extract metadata\n",
    "   - Store tables of metadata for variables in `input/vartables/*.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d2130-9524-489e-a2dd-3bd3e61e530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "# import pooch\n",
    "import json\n",
    "import pandas as pd\n",
    "import cdflib\n",
    "import urllib\n",
    "import zipfile\n",
    "from tempfile import NamedTemporaryFile\n",
    "import shutil\n",
    "# import datetime as dt\n",
    "from ftplib import FTP\n",
    "# from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21421c-8317-44b8-b5e3-63334b38dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_URL = \"https://docs.google.com/spreadsheets/d/e/2PACX-1vStz17Gi-O3tJjWcT_F0zYj4eCVuiiaU9ewpKTLlu_qRak-Cd0NHG3oQa0lcVFmWC2TFK3ecZHvdPxT/pub?output=xlsx\"\n",
    "CSV_PATH = os.path.abspath(\"input/overview.csv\")\n",
    "CSV_VARTABLES_PATH = os.path.abspath(\"input/vartables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b429f-801d-46e3-b553-517087e2f551",
   "metadata": {},
   "source": [
    "## Fetch Google Doc (products overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50be5e-cdb2-4c69-b85b-4465b97f552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_google_sheet(url=DOC_URL):\n",
    "    xl_doc = requests.get(url).content\n",
    "    overview = pd.read_excel(xl_doc, \"Overview\", header=1)\n",
    "    overview = overview.set_index(\"Name\").fillna(\"-\")\n",
    "    # names = list(overview.index.dropna())\n",
    "    # details = {}\n",
    "    # missing_sheets = []\n",
    "    # for name in names:\n",
    "    #     try:\n",
    "    #         details[name] = pd.read_excel(xl_doc, name).set_index(\"FIELD\", drop=False).fillna(\"-\")\n",
    "    #     except Exception:\n",
    "    #         missing_sheets.append(name)\n",
    "    #         details[name] = pd.DataFrame()\n",
    "    return overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18786d86-67eb-46ec-b4b2-e0ea8f054b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = load_google_sheet(DOC_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f684258-4406-4383-9e58-957b2a34332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36955c0-755d-4cfb-9ace-90e542c0da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanitise url and guess FTP link from it\n",
    "url_http = overview.get(\"Link: HTTP\").apply(lambda s: s.replace(\"%2F\", \"/\"))\n",
    "url_ftp_guessed = url_http.apply(lambda s: s.replace(\"#swarm/\", \"\").replace(\"https\", \"ftp\"))\n",
    "# Replace FTP links with the guessed one if there isn't one already\n",
    "for i in range(overview.shape[0]):\n",
    "    name = overview.iloc[i].name\n",
    "    if overview.iloc[i][\"Link: FTP\"] == \"-\":\n",
    "        overview.loc[name, \"Link: FTP\"] = url_ftp_guessed.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904f134-c68c-495c-b4e9-3be019fdf2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e4af0-e3d2-49b5-845d-abbb5b51995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview.to_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693eb54-1b2e-40a4-af65-c822c8bcc054",
   "metadata": {},
   "source": [
    "## Fetch VirES `product_types.json` (tables of metadata for variables in products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8459009-127b-41f5-9ec5-97a268406b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCT_TYPES_URL = \"https://raw.githubusercontent.com/ESA-VirES/VirES-Server/staging/vires/vires/data/product_types.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ed94c-400c-43d0-8ef3-59738d1b72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_product_types_json(url=PRODUCT_TYPES_URL):\n",
    "    json_content = json.loads(\n",
    "        requests.get(url).content\n",
    "    )\n",
    "    names = [content[\"name\"] for content in json_content]\n",
    "    product_metadata = {}\n",
    "    for name, product_content in zip(names, json_content):\n",
    "        product_metadata[name] = product_content\n",
    "    return product_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf831c30-70e7-4596-b032-0bbca2a48d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_metadata = load_product_types_json(PRODUCT_TYPES_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d02e55-376c-4353-b64c-22c48e0fe44d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mapping between names used in json file and our csv\n",
    "names_short_to_long = {\n",
    "    \"MODx_SC_1B\": 'SW_MODx_SC_1B',\n",
    "    \"MAGx_LR_1B\": 'SW_MAGx_LR_1B',\n",
    "    \"MAGx_HR_1B\": 'SW_MAGx_HR_1B',\n",
    "    \"EFIx_LP_1B\": 'SW_EFIx_LP_1B',\n",
    "    \"IBIxTMS_2F\": 'SW_IBIxTMS_2F',\n",
    "    \"EEFxTMS_2F\": 'SW_EEFxTMS_2F',\n",
    "    \"FACxTMS_2F\": 'SW_FACxTMS_2F',\n",
    "    \"TECxTMS_2F\": 'SW_TECxTMS_2F',\n",
    "    \"IPDxIRR_2F\": 'SW_IPDxIRR_2F',\n",
    "    # \"\": 'SW_AUX_IMF_2_',\n",
    "    # \"AEJxLPL_2F\": 'SW_AEJxLPL_2F',\n",
    "    # \"AEJxPBL_2F\": 'SW_AEJxPBL_2F',\n",
    "    # \"AEJxLPS_2F\": 'SW_AEJxLPS_2F',\n",
    "    # \"AEJxPBS_2F\": 'SW_AEJxPBS_2F',\n",
    "    # \"AOBxFAC_2F\": 'SW_AOBxFAC_2F',\n",
    "    # \"MITx_LP_2F\": 'SW_MITx_LP_2F',\n",
    "    # \"MITxTEC_2F\": 'SW_MITxTEC_2F',\n",
    "    # \"PPIxFAC_2F\": 'SW_PPIxFAC_2F',\n",
    "    # \"\": 'OMNI_HR_1min',\n",
    "    # \"AUX_OBSS2_\": 'SW_AUX_OBSx2_',\n",
    "    # \"AUX_OBSM2_\": 'SW_AUX_OBSx2_',\n",
    "    # \"AUX_OBSH2_\": 'SW_AUX_OBSH2_',\n",
    "    # \"VOBS_1M_2_\": 'SW_VOBS_xM_2_',\n",
    "    # \"VOBS_4M_2_\": 'SW_VOBS_xM_2_',\n",
    "    # \"\": 'GRACE_x_MAG',\n",
    "    # \"\": 'GFx_FGM_ACAL',\n",
    "    # \"\": 'CS_MAG'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a806b-1efa-4153-a361-874046e18d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each collection can contain subcollections\n",
    "for name, content in product_metadata.items():\n",
    "    print(content[\"datasets\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361ebb4-9ee0-4be8-80d1-998dff35d139",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Attempt to fetch example file for each product and extract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5a704-5fad-4467-a7f6-ef67bebebafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_host_link(link):\n",
    "    link = os.path.normpath(link).split(os.path.sep)\n",
    "    host = link[1]\n",
    "    directory = os.path.join(*link[2:])\n",
    "    return host, directory\n",
    "\n",
    "\n",
    "def find_file(host, directory, match=\"\"):\n",
    "    \"\"\"Try to find a file to use\"\"\"\n",
    "    # List files found in given directory\n",
    "    def check_dir(dir_):\n",
    "        print(\"Searching:\", dir_)\n",
    "        with FTP(host) as ftp:\n",
    "            ftp.login()\n",
    "            files = ftp.nlst(os.path.join(dir_, \"*.ZIP\"))\n",
    "        return files\n",
    "    if match==\"FAC_TMS_2F\":\n",
    "        files = check_dir(os.path.join(directory, \"Sat_AC\"))\n",
    "    else:\n",
    "        files = check_dir(os.path.join(directory, \"Sat_A\"))\n",
    "    if len(files) == 0:\n",
    "        files = check_dir(os.path.join(directory))\n",
    "        if len(files) == 0:\n",
    "            print(\"! No .ZIP files at:\", directory)\n",
    "            return None\n",
    "    if match != \"\":\n",
    "        # Reduce to subset containing \"match\"\n",
    "        files = [f for f in files if match in f]\n",
    "        # Screen out validation reports\n",
    "        files = [f for f in files if \"VAL_\" not in f]\n",
    "        if len(files) == 0:\n",
    "            print(f\"! No match found containing '{match}' and without 'VAL_'\")\n",
    "            return None\n",
    "    # Identify most recent file\n",
    "    files.sort()\n",
    "    matched_file = files[-1]\n",
    "    file_url = \"ftp://\" + os.path.join(host, matched_file)\n",
    "    print(\"Found:\", file_url)\n",
    "    return file_url\n",
    "\n",
    "\n",
    "# cdf_file_name_exceptions_match_pattern = {\n",
    "#     \"MAGx_CA_1B\": \"MDR_MAG\",\n",
    "#     \"MAGx_HR_1B\": \"MDR_MAG\",\n",
    "#     \"MAGx_LR_1B\": \"MDR_MAG\"\n",
    "# }\n",
    "\n",
    "\n",
    "def fetch_zipped_file(url, match=\"cdf\"):\n",
    "    \"\"\"Fetch a given file type from within an online zip file\"\"\"\n",
    "    output_file = NamedTemporaryFile()\n",
    "    zip_file, _ = urllib.request.urlretrieve(url)\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        file_names = zip_ref.namelist()\n",
    "        file_names_matched = [i for i in file_names if (match in i) or (match.upper() in i)]\n",
    "        if len(file_names_matched) == 0:\n",
    "            print(\"! Could not find match out of:\", file_names)\n",
    "            return None\n",
    "        if len(file_names_matched) > 1:\n",
    "            print(\"! Found too many:\", file_names_matched)\n",
    "            # Special case with MDR_MAG files\n",
    "            potential = [\n",
    "                i for i in file_names_matched if \"MDR_MAG\" in i\n",
    "            ]\n",
    "            if len(potential) == 1:\n",
    "                file_names_matched = potential\n",
    "        file_name = file_names_matched[0]\n",
    "        print(\"Using file:\", file_name)\n",
    "        with zip_ref.open(file_name) as f:\n",
    "            shutil.copyfileobj(f, output_file)\n",
    "            output_file.seek(0)\n",
    "    return output_file\n",
    "\n",
    "\n",
    "# def try_x_times(fn, x=3, delay_factor=10):\n",
    "#     \"\"\"Attempt function, fn, x times, with increasing delays\"\"\"\n",
    "#     for attempt in range(x):\n",
    "#         try:\n",
    "#             outputs = fn()\n",
    "#         except Exception as e:\n",
    "#             if attempt < x - 1:\n",
    "#                 delay = (attempt + 1) * delay_factor\n",
    "#                 print(f\"Failed ({e}). Trying again in {delay}s...\")\n",
    "#                 sleep(delay)\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 raise\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5894da8-35ea-4cd2-821b-9dbc95c1813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_attributes(cdf, varname) -> dict:\n",
    "    varatts = cdf.varattsget(varname)\n",
    "    description = varatts.get(\"DESCRIPTION\", \"-\")\n",
    "    units = varatts.get(\"UNITS\", \"-\")\n",
    "    dims = cdf.varinq(varname).get(\"Dim_Sizes\")\n",
    "    if dims == []:\n",
    "        dims = \"1\"\n",
    "    elif len(dims) == 1:\n",
    "        dims = str(dims[0])\n",
    "    else:\n",
    "        dims = str(dims)\n",
    "    type_ = cdf.varinq(varname).get(\"Data_Type_Description\")\n",
    "    return {\n",
    "        \"Name\": varname,\n",
    "        \"Units\": units,\n",
    "        \"Description\": description,\n",
    "        \"Dim\": dims,\n",
    "        \"Type\": type_\n",
    "    }\n",
    "\n",
    "\n",
    "def make_vartable(cdf) -> pd.DataFrame:\n",
    "    cdfinfo = cdf.cdf_info()\n",
    "    varnames = cdfinfo.get(\"zVariables\")\n",
    "    cols = [\"Units\", \"Description\", \"Dim\", \"Type\"]\n",
    "    product_varinfo = pd.DataFrame(columns=cols, index=varnames)\n",
    "    product_varinfo.index.name = \"Variable\"\n",
    "    for varname in varnames:\n",
    "        varatts = get_var_attributes(cdf, varname)\n",
    "        for col in cols:\n",
    "            product_varinfo.loc[varname, col] = varatts[col]\n",
    "    return product_varinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a541b-a188-4b86-be2c-c1fa5d3d5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_file_and_make_vartable(product_name):\n",
    "    # Find example file\n",
    "    ftp_link = overview.loc[product_name, \"Link: FTP\"]\n",
    "    if ftp_link[:3] != \"ftp\":\n",
    "        print(\"! No ftp link for:\", product_name)\n",
    "        return None\n",
    "    host, directory = split_host_link(ftp_link)\n",
    "    matchname = product_name.replace(\"x\", \"A\").replace(\"*\", \"\")\n",
    "    file_zip_url = find_file(host, directory, match=matchname)\n",
    "    # Download and open file, extracting the metadata\n",
    "    _file = fetch_zipped_file(file_zip_url, match=\"cdf\")\n",
    "    if not _file:\n",
    "        print(\"! Unable to find match for:\", product_name)\n",
    "        return None\n",
    "    cdf = cdflib.cdfread.CDF(_file.name, string_encoding='utf-8')\n",
    "    globalatts = cdf.globalattsget()\n",
    "    vartable = make_vartable(cdf)\n",
    "    return vartable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b65e2-23c2-400f-bdd8-da64ae5d2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_products = []\n",
    "products = overview.index\n",
    "# products = [\"MAGx_CA_1B\", \"MAGx_HR_1B\", \"MAGx_LR_1B\"]\n",
    "# products = [\"MCO_SHA_2C\", \"MMA_SHA_2F\", \"MCR_1DM2\"]\n",
    "# products = [\"FAC_TMS_2F\"]\n",
    "for product_name in products:\n",
    "    try:\n",
    "        vartable = find_file_and_make_vartable(product_name)\n",
    "        if vartable is not None:\n",
    "            vartable.to_csv(os.path.join(CSV_VARTABLES_PATH, f\"{product_name}.csv\"))\n",
    "            print(\"Saved:\", f\"{product_name}.csv\")\n",
    "    except Exception as e:\n",
    "        print(\"! Failed:\", product_name)\n",
    "        print(e)\n",
    "        failed_products.append(product_name)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:handbook]",
   "language": "python",
   "name": "conda-env-handbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
